services:
  # ===========================================================================
  # DATABASE SERVICES
  #
  # FIX 4.1: All credentials externalized to .env (see .env.example).
  #   Uses ${VAR:?error} syntax — compose fails fast if required vars missing.
  # ===========================================================================
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5433:5432"
    volumes:
      - postgres_cloudapp:/var/lib/postgresql/data
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  postgres-ml:
    image: postgres:15
    volumes:
      - "./init-ml.sql:/docker-entrypoint-initdb.d/init-ml.sql"
      - postgres_ml:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: ${POSTGRES_ML_USER}
      POSTGRES_PASSWORD: ${POSTGRES_ML_PASSWORD}
      POSTGRES_DB: ${POSTGRES_ML_DB}
    ports:
      - "5434:5432"
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_ML_USER} -d $${POSTGRES_ML_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  mysql:
    image: mysql:8.0
    container_name: mysql-petstore
    ports:
      - "3307:3306"
    volumes:
      - petstore:/var/lib/mysql
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
    networks:
      - backend
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  mongo:
    image: mongo
    container_name: mongo-chat
    ports:
      - '27018:27017'
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_DB}
    volumes:
      - mongo_chat:/data/db
    networks:
      - backend
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ===========================================================================
  # MESSAGING SERVICES
  # ===========================================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.2.3
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - backend
    healthcheck:
      # cp-zookeeper enables only "srvr" by default; "ruok" is disabled.
      test: ["CMD-SHELL", "echo srvr | nc -w 2 localhost 2181 | grep -q 'Mode:'"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  broker:
    image: confluentinc/cp-kafka:7.2.3
    container_name: kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_CREATE_TOPICS: 'chat'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9093,PLAINTEXT_HOST://host.docker.internal:9092,PLAINTEXT_INTERNAL://broker:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9093 --list || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # ===========================================================================
  # LLM Provider — Ollama via Docker profile
  #
  # For Apple Silicon GPU: install natively instead:
  #   brew install ollama && ollama pull qwen3:1.7b && ollama serve
  #
  # For containerized (no GPU):
  #   docker compose --profile ollama -f docker-compose-infrastructure.yml up -d
  # ===========================================================================
  ollama:
    container_name: ollama
    profiles: ["ollama"]
    build:
      context: ./
      dockerfile: Dockerfile_OLLAMA
      args:
        NEXT_PUBLIC_LLM_MODEL: ${LLM_MODEL}
        NEXT_PUBLIC_LLM_MODEL_EMBEDDING: ${LLM_MODEL_EMBEDDING}
    restart: unless-stopped
    ports:
      - 11434:11434
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:11434/api/tags || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s

  # ===========================================================================
  # OBSERVABILITY — Distributed Tracing (Jaeger v2)
  # ===========================================================================
  jaeger:
    image: jaegertracing/jaeger:2.15.0
    container_name: jaeger
    restart: unless-stopped
    ports:
      - "16686:16686"   # Jaeger UI
      - "4317:4317"     # OTLP gRPC receiver
      - "4318:4318"     # OTLP HTTP receiver
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - backend
    healthcheck:
      # jaeger:2.x image includes wget (not curl); health extension responds on /status.
      test: ["CMD-SHELL", "wget -qO- http://localhost:13133/status >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ===========================================================================
  # FIX 4.9: OBSERVABILITY — Metrics (Prometheus + Grafana)
  #
  # Prometheus scrapes Spring Boot Actuator /actuator/prometheus and Flask
  # metrics endpoints. Grafana provides dashboards.
  #
  # Prometheus UI: http://localhost:9090
  # Grafana UI:    http://localhost:3000 (admin / $GRAFANA_PASSWORD)
  # ===========================================================================
  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=7d'
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/-/healthy || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 3

  grafana:
    image: grafana/grafana:11.0.0
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
    networks:
      - backend

  # ===========================================================================
  # AI ORCHESTRATION INFRASTRUCTURE
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: redis-cache
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend

  mongodb-abtest:
    image: mongo:7
    container_name: mongodb-abtest
    restart: unless-stopped
    ports:
      - "27019:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ABTEST_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ABTEST_PASSWORD}
      MONGO_INITDB_DATABASE: ai_orchestration
      MONGO_ABTEST_ORCH_USER: ${MONGO_ABTEST_ORCH_USER:-orchestration_user}
      MONGO_ABTEST_ORCH_PASSWORD: ${MONGO_ABTEST_ORCH_PASSWORD:-orchestration_pass}
    volumes:
      - mongodb_data:/data/db
      - ./scripts/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend

  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    restart: unless-stopped
    ports:
      - "8123:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    healthcheck:
      # chromadb image has no curl/wget/nc; probe v2 heartbeat via bash /dev/tcp.
      test: ["CMD-SHELL", "bash -lc 'exec 3<>/dev/tcp/127.0.0.1/8000; printf \"GET /api/v2/heartbeat HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n\" >&3; grep -q \"200\" <&3'"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - backend

  # ===========================================================================
  # APPLICATION SERVICES (in infra compose)
  # ===========================================================================
  ai-orchestration-layer:
    build:
      context: ./ai-orchestration/ai-orchestration-layer
      dockerfile: Dockerfile
      args:
        PYTHONPATH: /app/src
        PYTHONUNBUFFERED: 1
    container_name: ai-orchestration-layer
    restart: unless-stopped
    environment:
      PYTHONPATH: /app/src
      PYTHONUNBUFFERED: 1
      ENVIRONMENT: ${ENVIRONMENT}
      OLLAMA_URL: ${OLLAMA_URL:-http://host.docker.internal:11434}
      LLM_TEMPERATURE: ${LLM_TEMPERATURE}
      LLM_MAX_TOKENS: ${LLM_MAX_TOKENS}
      INTERNAL_SERVICE_TOKEN: ${INTERNAL_SERVICE_TOKEN}
      CLOUDAPP_URL: ${CLOUDAPP_URL}
      PETSTORE_URL: ${PETSTORE_URL}
      VEHICLES_URL: ${VEHICLES_URL}
      ML_URL: ${ML_URL}
      # FIX 4.1: Credentials from .env — no more hardcoded connection strings
      POSTGRES_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      REDIS_URL: redis://redis:6379/0
      REDIS_MAX_CONNECTIONS: ${REDIS_MAX_CONNECTIONS}
      MONGODB_URL: mongodb://${MONGO_ABTEST_ORCH_USER}:${MONGO_ABTEST_ORCH_PASSWORD}@mongodb-abtest:27017/ai_orchestration?authSource=ai_orchestration
      MONGODB_DATABASE: ai_orchestration
      HTTP_TIMEOUT: ${HTTP_TIMEOUT}
      HTTP_MAX_CONNECTIONS: ${HTTP_MAX_CONNECTIONS}
      HTTP_MAX_KEEPALIVE: ${HTTP_MAX_KEEPALIVE}
      CHROMA_HOST: chromadb
      CHROMA_PORT: 8000
      CHROMA_COLLECTION: ${CHROMA_COLLECTION}
      CHROMA_PERSIST_DIR: /data/chroma
      RAG_SEARCH_K: ${RAG_SEARCH_K}
      ENABLE_CHECKPOINTING: ${ENABLE_CHECKPOINTING}
      ENABLE_HITL: ${ENABLE_HITL}
      ENABLE_PARALLEL: ${ENABLE_PARALLEL}
      ENABLE_ERROR_HANDLING: ${ENABLE_ERROR_HANDLING}
      ENABLE_AB_TESTING: ${ENABLE_AB_TESTING}
      LOG_LEVEL: ${LOG_LEVEL}
      LOG_FORMAT: ${LOG_FORMAT}
      # OpenTelemetry for FastAPI/Python (opt-in)
      OTEL_ENABLED: ${OTEL_ENABLED}
      OTEL_SERVICE_NAME: ai-orchestration
      OTEL_EXPORTER_OTLP_ENDPOINT: http://jaeger:4317
    volumes:
      - chroma_data:/data/chroma
      - ./logs:/app/logs
    depends_on:
      redis:
        condition: service_healthy
      chromadb:
        condition: service_healthy
      mongodb-abtest:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:8700/health || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - backend

  # ===========================================================================
  # AI MONITOR DASHBOARD
  # ===========================================================================
  ai-orchestration-monitor:
    build:
      context: ./ai-orchestration/ai-orchestration-monitor
      dockerfile: Dockerfile
      args:
        VITE_API_URL: http://localhost:80
        VITE_AI_BASE_URL: http://localhost:80/ai
        VITE_AI_WS_URL: ws://localhost:80/ai
        NODE_ENV: production
    container_name: ai-orchestration-monitor
    restart: unless-stopped
    ports:
      - "5010:5010"
    networks:
      - frontend

# ===========================================================================
# VOLUMES
# ===========================================================================
volumes:
  petstore:
  postgres_cloudapp:
    name: postgres-cloudapp-data
  postgres_ml:
    name: postgres-ml-data
  mongo_chat:
    name: mongo-chat-data
  redis_data:
    name: ai-orchestration-redis
  mongodb_data:
    name: ai-orchestration-mongodb
  chroma_data:
    driver: local
  prometheus_data:
    name: prometheus-data
  grafana_data:
    name: grafana-data

# ===========================================================================
# NETWORKS - Must match docker-compose-app.yml
# ===========================================================================
networks:
  frontend:
    name: app-frontend
  backend:
    name: app-backend
